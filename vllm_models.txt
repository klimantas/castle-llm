vLLM Local Models
Compatible models for local vLLM serving
==================================================

meta-llama/Llama-3.2-3B-Instruct
meta-llama/Llama-3.2-11B-Instruct
meta-llama/Llama-3.2-1B-Instruct
microsoft/Phi-3.5-mini-instruct
microsoft/Phi-3.5-MoE-instruct
mistralai/Mistral-7B-Instruct-v0.3
mistralai/Mistral-Nemo-Instruct-2407
teknium/OpenHermes-2.5-Mistral-7B
NousResearch/Hermes-2-Pro-Llama-3-8B
Qwen/Qwen2.5-7B-Instruct
Qwen/Qwen2.5-3B-Instruct
google/gemma-2-2b-it
google/gemma-2-9b-it

Note: To serve a model locally with vLLM:
vllm serve meta-llama/Llama-3.2-3B-Instruct --host 0.0.0.0 --port 8000